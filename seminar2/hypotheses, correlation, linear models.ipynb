{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Случайные величины и гипотезы\n",
    "\n",
    "Приведём пример генерации одной или нескольких случайных величин из [нормального](https://en.wikipedia.org/wiki/Normal_distribution) распределения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "print(norm.rvs(loc=0, scale=1, size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам часто будет встречаться *гистограмма*, поэтому разберёмся, что это такое:\n",
    "- по оси $0x$, как обычно, откладываются значения точек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация 10 случайных неповторяющихся значений из промежутка [0, 20)\n",
    "samples = np.random.choice(np.arange(0, 20, dtype=int), 10, replace=False)\n",
    "plt.scatter(samples, np.zeros_like(samples))\n",
    "plt.yticks([], [])\n",
    "plt.xticks(np.arange(0, 21, dtype=int), np.arange(0, 21, dtype=int))\n",
    "plt.xlabel('value')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $0x$ разбивается на некоторое число интервалов (параметр `bins`)\n",
    "- по оси $0y$ откладывается частота значений в каждом интервале"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples, bins=5, density=True)\n",
    "plt.xlabel('value')\n",
    "plt.ylabel('frequency of value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим гистограмму значений, сгенерированных из нормального распределения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number_of_samples =  100000\n",
    "plt.figure(figsize = (14, 6))\n",
    "for mean, std in [(0, 1), (0, 2), (2, 1)]:\n",
    "    plt.hist(norm.rvs(loc=mean, scale=std, size=number_of_samples), bins=50, density=True, label='N' + str((mean, std)))\n",
    "    plt.plot([mean] * 10, np.linspace(0, 0.5, 10), label='mean = ' + str(mean))\n",
    "plt.xlabel('value')\n",
    "plt.ylabel('frequency of value')\n",
    "plt.plot(1, 0.01, 'bv', markersize=10, label='some value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что точки нормального распределения в основном концентрируются вокруг среднего значения (`mean`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка гипотез\n",
    "Допустим, мы хотим проверить, правда ли некоторая точка $x$ принадлежит выборке, сгенерированной из нормального распределения с заданными параметрами.\n",
    "\n",
    "Напомним механизм проверки гипотез на примере описанной задачи:\n",
    "- выдвигается гипотеза $H_0$: $x$ получена из заданного распределения\n",
    "- альтернативная к ней $H_1$: $x$ получена из распределения с какими-то другими параметрами\n",
    "- в точке $x$ вычисляется *p-value* aka ошибка $1$-го рода: вероятность того, что для точки $x$ гипотеза $H_0$ верна, а мы в неё не верим\n",
    "- *p-value* сравнивается с заданным уровнем значимости, например $0.05$ -- чем он меньше, тем меньше мы хотим ошибиться\n",
    "\n",
    "*p-value* для нормального распределения соответствует площади \"хвоста\", в который точка попала:\n",
    "![](https://statistics.laerd.com/statistical-guides/img/normal-1.png)\n",
    "\n",
    "Попробуем это закодить:\n",
    "1. Задайте произвольные параметры нормального распределения.\n",
    "2. Выберите точку, гипотезу о распределении которой будем проверять.\n",
    "3. Задайте уровень значимости и вычислите *p-value* для выбранной точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples =  100000\n",
    "\n",
    "# определите параметры распределения\n",
    "mean = #YOUR CODE HERE\n",
    "std = #YOUR CODE HERE\n",
    "\n",
    "# точка, которую будем проверять\n",
    "observed_value = #YOUR CODE HERE\n",
    "p_value = #YOUR CODE HERE\n",
    "significance = #YOUR CODE HERE\n",
    "if #YOUR CODE HERE\n",
    "    print('Reject hypothesis')\n",
    "else:\n",
    "    print('We cannot reject or accept hypothesis')\n",
    "\n",
    "# визуализация заданного распределения\n",
    "## генерация выборки из заданного распределения\n",
    "sample = norm.rvs(loc=mean, scale=std, size=number_of_samples)\n",
    "## вычисление реальной плотности\n",
    "density_domain = np.linspace(sample.min(), sample.max(), 100)\n",
    "density = norm.pdf(density_domain)\n",
    "\n",
    "plt.figure(figsize = (14, 6))\n",
    "plt.hist(sample, bins = 50, density=True, label='approximate distribution')\n",
    "plt.plot(observed_value, norm.pdf(observed_value), 'ro', label='observation')\n",
    "plt.plot(density_domain, density, label='real distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Корреляция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://idatassist.com/wp-content/uploads/2017/04/dreamstime_m_37904189-610x461.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним определение коэффициента корреляции между векторами $x = (x_1, \\ldots, x_n)$ и $y = (y_1, \\ldots, y_n)$:\n",
    "\n",
    "$$\n",
    "  \\rho = \\frac{\\sum_{i=1}^n (x_i - \\overline x)(y_i - \\overline y)}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline x)^2} \\sqrt{\\sum_{i=1}^n (y_i - \\overline y)^2}}\n",
    "$$\n",
    "\n",
    "Более подробное описание есть в лекционных [слайдах](https://github.com/burrita/datacultureMO/blob/master/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D1%8F%201.pdf).\n",
    "\n",
    "$\\textit{Task:}$ Создайте 3 вектора. Первый и второй должны иметь корреляцию больше 0.5, первый и третий меньше -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_1 = #YOUR CODE HERE\n",
    "vector_2 = #YOUR CODE HERE\n",
    "vector_3 = #YOUR CODE HERE\n",
    "\n",
    "assert len(vector_1) == len(vector_2) == len(vector_3), \"Векторы разной длины\"\n",
    "\n",
    "v1_v2_corr = #YOUR CODE HERE\n",
    "v1_v3_corr = #YOUR CODE HERE\n",
    "\n",
    "assert v1_v2_corr>0.5, \"Первый и второй векторы должны иметь корреляцию больше 0.5. У вас: \" + str(round(v1_v2_corr,2))\n",
    "assert v1_v3_corr<0.5, \"Первый и третий векторы должны иметь корреляцию меньше -0.5. У вас: \" + str(round(v1_v3_corr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation refers to $\\textbf{only linear}$ relationships between two variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 100, 0.1)\n",
    "y = np.sin(x)\n",
    "print(np.corrcoef(x,y)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробное описание можно найти в лекционных [слайдах](https://github.com/burrita/datacultureMO/blob/master/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D1%8F2.pdf).\n",
    "\n",
    "**TL;DR**\n",
    "\n",
    "Попытаемся предсказать (объяснить) величину $y$ через набор характеристик $x_1, \\ldots, x_n$.\n",
    "\n",
    "Для этого можно построить линейную модель:\n",
    "$$y = \\alpha + \\beta_1x_1 + \\beta_2x_2+...+\\beta_nx_n + \\varepsilon$$\n",
    "$\\varepsilon$ -- нормально распределённый шум.\n",
    "\n",
    "Подбор параметров $\\alpha, \\beta_1, \\ldots, \\beta_n$ происходит за счёт минимизации суммарной ошибки:\n",
    "$$MSE = \\sum (y_i - y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сгенерированные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from numpy.random import normal\n",
    "\n",
    "number_of_samples = 250\n",
    "\n",
    "X1 = normal(loc=0.0, scale=1, size=number_of_samples)\n",
    "X2 = normal(loc=0.0, scale=1, size=number_of_samples)\n",
    "\n",
    "# сгенерируйте нормально распределённый шум\n",
    "noise = # YOUR CODE HERE\n",
    "\n",
    "# задайте любое уравнение регрессии с дополнительным зашумлением значений\n",
    "Y = # YOUR CODE HERE\n",
    "\n",
    "## Adding noise for variables\n",
    "X1 += normal(loc=0.0, scale=0, size=number_of_samples)\n",
    "X2 += normal(loc=0.0, scale=0, size=number_of_samples)\n",
    "\n",
    "X = np.column_stack([X1, X2])\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), sharey=True)\n",
    "\n",
    "x_axis = np.arange(min(X1),max(X1), 0.1)\n",
    "step = max(X1) - min(X1)\n",
    "ax1.scatter(X1, Y, edgecolors=(0, 0, 0))\n",
    "ax1.plot(x_axis, results.params[0] + results.params[1] * x_axis, 'k--', lw=5, c='red')\n",
    "ax1.set_ylim(np.mean(Y) - step, np.mean(Y) + step)\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('Y')\n",
    "\n",
    "x_axis = np.arange(min(X2), max(X2), 0.1)\n",
    "step = max(X2) - min(X2)\n",
    "ax2.scatter(X2, Y, edgecolors=(0, 0, 0))\n",
    "ax2.plot(x_axis, results.params[0] + results.params[2] * x_axis, 'k--', lw=5, c='red')\n",
    "ax2.set_ylim(np.mean(Y) - step, np.mean(Y) + step)\n",
    "ax2.set_xlabel('X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/t0zit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://vitalflux.com/wp-content/uploads/2015/02/fittings.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://theneural.files.wordpress.com/2011/07/valid2.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy.random import normal\n",
    "np.random.seed(67)\n",
    "\n",
    "number_of_samples = 1000\n",
    "X1 = normal(loc=0.0, scale=1, size=number_of_samples)\n",
    "noise = normal(loc=0.0, scale=0.1, size=number_of_samples)\n",
    "Y = 1 + X1  + noise\n",
    "\n",
    "variables_list = [X1]\n",
    "for i in range(100):\n",
    "    variables_list.append(normal(loc=0.0, scale=0.1, size=number_of_samples))\n",
    "\n",
    "X = np.column_stack(variables_list)\n",
    "ntr = int(0.5 * X.shape[0])\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X[:ntr,:], Y[:ntr])\n",
    "Y_predicted = model.predict(X)\n",
    "train_error = mean_squared_error(Y_predicted[:ntr], Y[:ntr])\n",
    "val_error = mean_squared_error(Y_predicted[ntr:], Y[ntr:])\n",
    "print('Training Error:  \\t',train_error)\n",
    "print('Validation Error:\\t',mean_squared_error(Y_predicted[ntr:], Y[ntr:]))\n",
    "print('diff:\\t\\t\\t', val_error - train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus task\n",
    "\n",
    "Визуализируем эффект переобучения. Генерация данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 1000\n",
    "X1 = normal(loc=0.0, scale=1, size=number_of_samples)\n",
    "noise = normal(loc=0.0, scale=1.2, size=number_of_samples)\n",
    "Y = 1 + X1 + noise\n",
    "plt.plot(X1, Y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем приближать данные разными степенями полинома:\n",
    "$$\n",
    "  y(x) = a_0 + a_1x + \\ldots + a_nx^n\n",
    "$$\n",
    "\n",
    "Поэкспериментируйте с разными значениями степени и проанализируйте результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 1000\n",
    "X1 = normal(loc=0.0, scale=1, size=number_of_samples)\n",
    "noise = normal(loc=0.0, scale=1.2, size=number_of_samples)\n",
    "Y = 1 + X1 + noise\n",
    "\n",
    "\n",
    "max_power = # YOUR CODE HERE\n",
    "\n",
    "variables_list = []\n",
    "for i in range(max_power):\n",
    "    variables_list.append(X1 ** i)\n",
    "\n",
    "X = np.column_stack(variables_list)\n",
    "ntr = int(0.5 * X.shape[0])\n",
    "\n",
    "# определение линейной модели\n",
    "model = # YOUR CODE HERE\n",
    "\n",
    "# обучение заданной модели на обучающей (первые ntr строк) выборке\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# предсказание ответов для всей выборки\n",
    "Y_predicted = # YOUR CODE HERE\n",
    "\n",
    "train_error = mean_squared_error(Y_predicted[:ntr], Y[:ntr])\n",
    "val_error = mean_squared_error(Y_predicted[ntr:], Y[ntr:])\n",
    "print('Training Error:  \\t',train_error)\n",
    "print('Validation Error:\\t',mean_squared_error(Y_predicted[ntr:], Y[ntr:]))\n",
    "print('diff:\\t\\t\\t', val_error - train_error)\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "ax1.plot(X1, Y, 'o')\n",
    "ax1.set_ylim(Y.min(), Y.max())\n",
    "\n",
    "ax2.plot(X1[:ntr], Y[:ntr], 'o')\n",
    "ax2.plot(X1[ntr:], Y_predicted[ntr:], 'o')\n",
    "# comment the line below to see all dots\n",
    "ax2.set_ylim(Y.min(), Y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Реальные данные\n",
    "Stop playing games, it's real world now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия\n",
    "Загрузим данные из стандартного набора библиотеки `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "data = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "data['PRICE'] = boston.target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "Y = # YOUR CODE HERE\n",
    "X = # YOUR CODE HERE\n",
    "X = sm.add_constant(X)\n",
    "ols_model = sm.OLS(Y, X)\n",
    "ols_results = ols_model.fit()\n",
    "print(ols_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "print(breast_cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(breast_cancer.data[:,:28], columns = breast_cancer.feature_names[:28])\n",
    "data['CANCER'] = breast_cancer.target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "Y = # YOUR CODE HERE\n",
    "X = # YOUR CODE HERE\n",
    "X = sm.add_constant(X)\n",
    "model = sm.Logit(Y, X)\n",
    "results = model.fit()\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus task\n",
    "\n",
    "Постройте те же модели для классификации и регрессии с использованием методов из `scikit-learn`.\n",
    "- [Линейная регрессия](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) (для решения задачи *регрессии*)\n",
    "- [Логистическая регрессия](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (для решения задачи *классификации*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
